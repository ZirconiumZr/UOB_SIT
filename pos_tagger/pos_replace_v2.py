# -*- coding: utf-8 -*-
"""pos_replace_v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mdfqrn2Nc8xE6xFlAstCmzPeEMCgguP-
"""

import nltk
from nltk import word_tokenize, pos_tag
import pandas as pd
import io

try:
  nltk.data.find('tokenizers/punkt')
except LookupError:
  nltk.download('punkt')

try:
  nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
  nltk.download('averaged_perceptron_tagger')

try:
  nltk.data.find('taggers/universal_tagset')
except LookupError:
  nltk.download('universal_tagset')

template = pd.read_csv('/content/replace_template.csv')

# template=template.fillna('')

text1 = "the warmer areas i am at your bb was he who does that is"

text2 = "at your bb from your obey for you will be"

text3 = "i'm fine and you"

text4 = "i'm doing great what can i do for you today"

text5 = "at your bb i'm fine and you"

text6 = "at your bb in your bb for you will be manage you will be"

text7 = "manage you will be i'm"

def pos_replace(l,template):
  words = word_tokenize(l)
  l_r = " ".join(words)
  for i in range(len(template)):
    row = template.iloc[i]
    pos_before = str(row['pos_before']).split(';')
    pos_after = str(row['pos_after']).split(';')
    replace_list = str(row['replace']).split(';')
    key = word_tokenize(row['key'])
    for p in replace_list:
      p_w = word_tokenize(p)
      p_s = " ".join(p_w)
      if p_s in l_r:
        # p_w = word_tokenize(p)
        lenth_p = len(p_w)
        words = word_tokenize(l_r)
        words_pos = pos_tag(words, tagset='universal')
        lenth_w = len(words)
        for i in range(lenth_w-lenth_p+1):
          if words[i:i+lenth_p]==p_w:
            if words_pos[i-1][1] in pos_before:
              print('the pos of the word before', p, 'is',words_pos[i-1][1])
              words[i:i+lenth_p] = key
              words_pos = pos_tag(words, tagset='universal')
              lenth_w = len(words)
            # l_r=l_r.replace(p, row[0])
            # l_r = " ".join(words)
            # print(l_r)
            elif i+lenth_p<len(words) and words_pos[i+lenth_p][1] in pos_after:
              print('the pos of the word after', p, 'is',words_pos[i+lenth_p][1])
              words[i:i+lenth_p] = key
              words_pos = pos_tag(words, tagset='universal')
              lenth_w = len(words)
              # l_r=l_r.replace(p, row[0])
              # l_r = " ".join(words)
              # print(l_r)
        l_r = " ".join(words)
        print(l_r)
        # for i in range(lenth_w-lenth_p+1):
        #   print(words[i:i+lenth_p])
        ## index_ = [i for i in range(lenth_w-lenth_p+1) if words[i:i+lenth_p]==p_w]
        # print(index_)
        # y = p.replace(' ','')
        # interim = l_r.replace(p, y)
        # words = word_tokenize(interim)
        # interim_pos = pos_tag(words, tagset='universal')
        # index_=[i for i,x in enumerate(interim_pos) if x[0]==y]

        # for i in index_:
        #   if words_pos[i-1][1] in pos_before:
        #     print('the pos of the word before', p, 'is',words_pos[i-1][1])
        #     words[i:i+lenth_p] = key
        #     # l_r=l_r.replace(p, row[0])
        #     l_r = " ".join(words)
        #     print(l_r)
        #   elif words_pos[i+lenth_p][1] in pos_after:
        #     print('the pos of the word after', p, 'is',words_pos[i+lenth_p][1])
        #     words[i:i+lenth_p] = key
        #     # l_r=l_r.replace(p, row[0])
        #     l_r = " ".join(words)
        #     print(l_r)
  return l_r

pos_replace(text1,template)

pos_replace(text2,template)

pos_replace(text3,template)

pos_replace(text4,template)

pos_replace(text5,template)

pos_replace(text6,template)

pos_replace(text7,template)