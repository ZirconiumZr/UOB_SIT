# -*- coding: utf-8 -*-
"""pos_replace_v5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mdfqrn2Nc8xE6xFlAstCmzPeEMCgguP-
"""

import nltk
from nltk import word_tokenize, pos_tag
import pandas as pd
import io 
from nltk.tokenize.treebank import TreebankWordDetokenizer

try:
  nltk.data.find('tokenizers/punkt')
except LookupError:
  nltk.download('punkt')

try:
  nltk.data.find('taggers/averaged_perceptron_tagger')
except LookupError:
  nltk.download('averaged_perceptron_tagger')

try:
  nltk.data.find('taggers/universal_tagset')
except LookupError:
  nltk.download('universal_tagset')

template = pd.read_csv('/content/replace_template.csv')

# template=template.fillna('')

text1 = "i'm fine at your bb and u o p is a bank"

text2 = "i'm from uob and i'm fine in u o b"

text3 = "you can get youll be credit card"

swear_words=['arse','ass','asshole',
             'bastard','bitch','bollocks','brotherfucker','bugger','bullshit',
             'childfucker','cocksucker','crap','cunt',
             'damn',
             'effing',
             'fatherfucker','frigger','fuck',
             'goddamn','godsdamn',
             'hell','horseshit',
             'motherfucker',
             'nigga',
             'piss','prick',
             'shit','sisterfucker','slut',
             'twat']

def pos_replace(l, template):
    '''
    l: the stt sentence
    template: DataFrame, read as csv from stt_replace_template.txt
    pos_tag reference: https://universaldependencies.org/u/pos/all.html#al-u-pos/
    '''
    flag_change = ""
    words = word_tokenize(l)
    words = [w if w not in swear_words else '*'*len(w) for w in words]
    # l_r = TreebankWordDetokenizer().detokenize(words)
    l_r = " ".join(words)
    for i in range(len(template)):
        row = template.iloc[i]
        pos_before = str(row['pos_before']).split(';')
        pos_after = str(row['pos_after']).split(';')
        replace_list = str(row['replace']).split(';')
        flag_direct_replace = str(row['flag_direct_replace'])
        key = word_tokenize(row['key'])           
        for p in replace_list:
            p_w = word_tokenize(p)
            # p_s = TreebankWordDetokenizer().detokenize(p_w)
            p_s = " ".join(p_w)
            if p_s in l_r:
                # p_w = word_tokenize(p)
                lenth_p = len(p_w)
                words = word_tokenize(l_r)
                words_pos = pos_tag(words, tagset='universal')
                lenth_w = len(words)
                # r_len = lenth_w-lenth_p+1
                if flag_direct_replace.lower() == "x":
                    i = 0
                    # for i in range(lenth_w-lenth_p+1):
                    while i < lenth_w-lenth_p+1:
                        # print(i)
                        if words[i:i+lenth_p]==p_w:
                            print('directly replace', p, 'with',row[0])
                            words[i:i+lenth_p] = key
                            words_pos = pos_tag(words, tagset='universal')
                            lenth_w = len(words)
                            # r_len = lenth_w-lenth_p+1
                            flag_change = "x"
                        i += 1
                else:
                    # for i in range(lenth_w-lenth_p+1):
                    i = 0
                    while i < lenth_w-lenth_p+1:
                        # print(i)
                        if words[i:i+lenth_p]==p_w:
                            if words_pos[i-1][1] in pos_before:
                                print('the pos of the word before', p, 'is',words_pos[i-1][1])
                                words[i:i+lenth_p] = key
                                words_pos = pos_tag(words, tagset='universal')
                                lenth_w = len(words)
                                # r_len = lenth_w-lenth_p+1
                                flag_change = "x"
                                # l_r=l_r.replace(p, row[0])
                                # l_r = " ".join(words)
                                # print(l_r)
                            elif i+lenth_p<len(words) and words_pos[i+lenth_p][1] in pos_after:
                                print('the pos of the word after', p, 'is',words_pos[i+lenth_p][1])
                                words[i:i+lenth_p] = key
                                words_pos = pos_tag(words, tagset='universal')
                                lenth_w = len(words)
                                # r_len = lenth_w-lenth_p+1
                                flag_change = "x"
                                # l_r=l_r.replace(p, row[0])
                                # l_r = " ".join(words)
                                # print(l_r)
                        i += 1
                if flag_change == "x":
                    print(l_r)
                    l_r = " ".join(words)
                    # l_r = TreebankWordDetokenizer().detokenize(words)
                    print(l_r)
                    flag_change = ""
               
        
    # l_r = l
    # for i in range(len(template)):   # get every row in the template
    #     row = template.iloc[i]
    #     pos_before = str(row['pos_before']).split(';')
    #     pos_after = str(row['pos_after']).split(';')
    #     replace_list = str(row['replace']).split(';')
    #     print('replace_list:',replace_list)
    #     for p in replace_list:   # get each element in replace_list of template row[i]
    #         print('find p for=', p)
    #         if p in l_r:
    #             print('find p if=', p)
    #             # y = p.replace(' ','')
    #             y = re.sub(r'[^\w\s]','',p)  # replace punctuations to blank str
    #             print('y: ',y)
    #             interim = l_r.replace(p, y)
    #             words = word_tokenize(interim)
    #             interim_pos = pos_tag(words, tagset='universal')
    #             index_=[i for i,x in enumerate(interim_pos) if x[0]==y]
    #             for i in index_:
    #                 if interim_pos[i-1][1] in pos_before:
    #                     print('the pos of the word before', p, 'is',interim_pos[i-1][1])
    #                     words[i] = row[0]
    #                     # l_r=l_r.replace(p, row[0])
    #                     l_r = " ".join(words)
    #                     print(l_r)
    #                 elif interim_pos[i+1][1] in pos_after:
    #                     print('the pos of the word after', p, 'is',interim_pos[i+1][1])
    #                     words[i] = row[0]
    #                     # l_r=l_r.replace(p, row[0])
    #                     l_r = " ".join(words)
    #                     print(l_r)
    return l_r

pos_replace(text1, template)

pos_replace(text2, template)

pos_replace(text3, template)
